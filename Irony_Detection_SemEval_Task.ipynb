{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Comments for the Task </h2>\n",
    "\n",
    "The <i>f-score</i> of the baseline system is 0.6263 (default SVM was used). It was decided to use two \"tactics\" to exceed this score:\n",
    "<ul>\n",
    "    <li>Use three additional classifiers, Random Forest, Decision Tree and improved SVM (with SGD training, SGDClassifier)</li>\n",
    "    <li>Change the number of n-grams in the Tf-Idf vectorizer</li>\n",
    "</ul>\n",
    "\n",
    "It has been found that the effective number of n-grams is 6 (6-grams). In that case the <i>f-scores</i> are:\n",
    "<ol>\n",
    "    <li>with SVM (the same with SGD): 0.6671</li>\n",
    "    <li>with RF: 0.6666</li>\n",
    "    <li>with DT: 0.6660</li>\n",
    "</ol>\n",
    "\n",
    "With unigrams results of RF, DT and SGD are worse than baseline's SVM: 0.5563, 0.5727 and 0.6191 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import logging\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dataset(fp):\n",
    "    '''\n",
    "    Loads the dataset .txt file with label-tweet on each line and parses the dataset.\n",
    "    :param fp: filepath of dataset\n",
    "    :return:\n",
    "        corpus: list of tweet strings of each tweet.\n",
    "        y: list of labels\n",
    "    '''\n",
    "    y = []\n",
    "    corpus = []\n",
    "    with open(fp, 'rt', encoding='utf-8') as data_in:\n",
    "        for line in data_in:\n",
    "            if not line.lower().startswith(\"tweet index\"): # discard first line if it contains metadata\n",
    "                line = line.rstrip() # remove trailing whitespace\n",
    "                label = int(line.split(\"\\t\")[1])\n",
    "                tweet = line.split(\"\\t\")[2]\n",
    "                y.append(label)\n",
    "                corpus.append(tweet)\n",
    "\n",
    "    return corpus, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(corpus):\n",
    "    '''\n",
    "    Tokenizes and creates TF-IDF BoW vectors.\n",
    "    :param corpus: A list of strings each string representing document.\n",
    "    :return: X: A sparse csr matrix of TFIDF-weigted ngram counts.\n",
    "    '''\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\",\n",
    "                                ngram_range=(6,6))\n",
    "    #vectorizer = TfidfVectorizer(strip_accents=\"unicode\", analyzer=\"word\", tokenizer=tokenizer, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    # print(vectorizer.get_feature_names()) # to manually check if the tokens are reasonable\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1923], [1, 1911]]\n",
      "F1-score Task A 0.626344086022\n"
     ]
    }
   ],
   "source": [
    "# Dataset: SemEval2018-T4-train-taskA.txt or SemEval2018-T4-train-taskB.txt\n",
    "DATASET_FP = \"./SemEval2018-T4-train-taskA.txt\"\n",
    "TASK = \"A\" # Define, A or B\n",
    "FNAME = './predictions-task' + TASK + '.txt'\n",
    "PREDICTIONSFILE = open(FNAME, \"w\", encoding='utf-8')\n",
    "\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "CLF = LinearSVC() # the default, non-parameter optimized linear-kernel SVM\n",
    "\n",
    "# Loading dataset and featurised simple Tfidf-BoW model\n",
    "corpus, y = parse_dataset(DATASET_FP)\n",
    "X = featurize(corpus)\n",
    "\n",
    "class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "print (class_counts)\n",
    "    \n",
    "# Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    \n",
    "# Modify F1-score calculation depending on the task\n",
    "if TASK.lower() == 'a':\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "elif TASK.lower() == 'b':\n",
    "    score = metrics.f1_score(y, predicted, average=\"macro\")\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "for p in predicted:\n",
    "    PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1923], [1, 1911]]\n",
      "F1-score Task A 0.556307692308\n"
     ]
    }
   ],
   "source": [
    "# Dataset: SemEval2018-T4-train-taskA.txt or SemEval2018-T4-train-taskB.txt\n",
    "DATASET_FP = \"./SemEval2018-T4-train-taskA.txt\"\n",
    "TASK = \"A\" # Define, A or B\n",
    "FNAME = './predictions-task' + TASK + '.txt'\n",
    "PREDICTIONSFILE = open(FNAME, \"w\", encoding='utf-8')\n",
    "\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "CLF = RandomForestClassifier()\n",
    "\n",
    "# Loading dataset and featurised simple Tfidf-BoW model\n",
    "corpus, y = parse_dataset(DATASET_FP)\n",
    "X = featurize(corpus)\n",
    "\n",
    "class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "print (class_counts)\n",
    "    \n",
    "# Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    \n",
    "# Modify F1-score calculation depending on the task\n",
    "if TASK.lower() == 'a':\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "elif TASK.lower() == 'b':\n",
    "    score = metrics.f1_score(y, predicted, average=\"macro\")\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "for p in predicted:\n",
    "    PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1923], [1, 1911]]\n",
      "F1-score Task A 0.572747014115\n"
     ]
    }
   ],
   "source": [
    "# Dataset: SemEval2018-T4-train-taskA.txt or SemEval2018-T4-train-taskB.txt\n",
    "DATASET_FP = \"./SemEval2018-T4-train-taskA.txt\"\n",
    "TASK = \"A\" # Define, A or B\n",
    "FNAME = './predictions-task' + TASK + '.txt'\n",
    "PREDICTIONSFILE = open(FNAME, \"w\", encoding='utf-8')\n",
    "\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "CLF = DecisionTreeClassifier()\n",
    "\n",
    "# Loading dataset and featurised simple Tfidf-BoW model\n",
    "corpus, y = parse_dataset(DATASET_FP)\n",
    "X = featurize(corpus)\n",
    "\n",
    "class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "print (class_counts)\n",
    "    \n",
    "# Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    \n",
    "# Modify F1-score calculation depending on the task\n",
    "if TASK.lower() == 'a':\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "elif TASK.lower() == 'b':\n",
    "    score = metrics.f1_score(y, predicted, average=\"macro\")\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "for p in predicted:\n",
    "    PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1923], [1, 1911]]\n",
      "F1-score Task A 0.667133111772\n"
     ]
    }
   ],
   "source": [
    "# Dataset: SemEval2018-T4-train-taskA.txt or SemEval2018-T4-train-taskB.txt\n",
    "DATASET_FP = \"./SemEval2018-T4-train-taskA.txt\"\n",
    "TASK = \"A\" # Define, A or B\n",
    "FNAME = './predictions-task' + TASK + '.txt'\n",
    "PREDICTIONSFILE = open(FNAME, \"w\", encoding='utf-8')\n",
    "\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "CLF = linear_model.SGDClassifier()\n",
    "\n",
    "# Loading dataset and featurised simple Tfidf-BoW model\n",
    "corpus, y = parse_dataset(DATASET_FP)\n",
    "X = featurize(corpus)\n",
    "\n",
    "class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "print (class_counts)\n",
    "    \n",
    "# Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "    \n",
    "# Modify F1-score calculation depending on the task\n",
    "if TASK.lower() == 'a':\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "elif TASK.lower() == 'b':\n",
    "    score = metrics.f1_score(y, predicted, average=\"macro\")\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "for p in predicted:\n",
    "    PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
