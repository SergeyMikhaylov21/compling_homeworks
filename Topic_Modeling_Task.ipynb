{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('4_en.txt', encoding='utf-8') as data:\n",
    "    docs = data.read().splitlines()\n",
    "docss = []\n",
    "for i in range(len(docs)):\n",
    "    if docs[i] != '':\n",
    "        docss.append(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text collection size and median length in symbols:\n",
      "4276 164.0\n"
     ]
    }
   ],
   "source": [
    "n_topics = 20\n",
    "print('Text collection size and median length in symbols:')\n",
    "print(len(docss), np.median([len(d) for d in docss]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                   ngram_range=(2,2),\n",
    "                                   min_df=5,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docss)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF doc-topic shape: (4276, 20)\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics)\n",
    "nmf_doc_topic = nmf.fit_transform(tfidf)\n",
    "print('NMF doc-topic shape:', nmf_doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA on raw words counts\n",
    "tf_vectorizer = CountVectorizer(max_df=0.8,\n",
    "                                ngram_range=(2,2),\n",
    "                                min_df=5,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docss)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Сергей\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA doc-topic shape: (4276, 20)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=20)\n",
    "lda_doc_topic = lda.fit_transform(tf)\n",
    "print('LDA doc-topic shape:', lda_doc_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMF top terms:\n",
      "Topic 0:\n",
      "don know, know doing, know don, know going, know ve, know happening, really don, know think, know know, just don\n",
      "Topic 1:\n",
      "united states, president obama, states going, american people, president united, mexican government, iran deal, donald trump, september 11th, trillions dollars\n",
      "Topic 2:\n",
      "hillary clinton, radical islam, clinton campaign, radical islamic, say words, burden hillary, clinton tell, secretary state, islamic terrorism, words radical\n",
      "Topic 3:\n",
      "don want, want money, said don, want tell, want know, want don, want want, want people, don need, money don\n",
      "Topic 4:\n",
      "make america, america great, great going, right thing, ladies gentlemen, doing know, months ago, want make, white house, going crazy\n",
      "Topic 5:\n",
      "ve seen, illegal immigration, seen like, like ve, ships ve, los angeles, know ve, mean ve, common core, happened country\n",
      "Topic 6:\n",
      "ve got, got smart, say ve, got ve, got stop, politically correct, nice guy, mean ve, got make, got know\n",
      "Topic 7:\n",
      "going happen, happen going, happen anymore, people come, good going, going let, just going, going come, things going, illegal immigrants\n",
      "Topic 8:\n",
      "think going, don think, going great, going use, going really, going think, going doing, new york, going going, iowa think\n",
      "Topic 9:\n",
      "going win, win going, going going, said going, win win, don win, say oh, win just, going knock, going start\n",
      "Topic 10:\n",
      "years ago, seven years, build wall, 15 years, 12 years, long time, 000 miles, 20 years, number years, everybody wanted\n",
      "Topic 11:\n",
      "second amendment, going care, common core, going protect, just remember, know know, man great, lot things, care vets, amendment going\n",
      "Topic 12:\n",
      "going make, make country, country great, trade deals, make great, social security, country rich, lot money, going bring, country strong\n",
      "Topic 13:\n",
      "didn want, said oh, want credit, saudi arabia, politically correct, good things, don forget, don care, didn work, million people\n",
      "Topic 14:\n",
      "ll tell, know ll, tell going, don like, radical islam, 000 people, lot money, millions dollars, elected president, going build\n",
      "Topic 15:\n",
      "thank thank, thank everybody, everybody thank, want thank, love thank, great honor, love love, long time, really amazing, okay thank\n",
      "Topic 16:\n",
      "foreign policy, middle east, lot money, president obama, nuclear weapons, american people, cold war, destabilize middle, total disaster, long time\n",
      "Topic 17:\n",
      "new hampshire, south carolina, just came, 000 people, leading big, don care, poll just, iowa new, going stop, great people\n",
      "Topic 18:\n",
      "little bit, millions millions, millions dollars, donald trump, weeks ago, think good, say going, don think, south korea, didn know\n",
      "Topic 19:\n",
      "people don, lot people, don like, don think, know doing, don understand, know know, want thank, know people, people lot\n",
      "\n",
      "LDA top terms:\n",
      "Topic 0:\n",
      "going make, ll tell, middle east, 150 billion, said don, billion year, trade deficit, republican party, tell going, days ago\n",
      "Topic 1:\n",
      "going happen, ve seen, build wall, thank thank, going pay, going build, happen going, thank everybody, pay wall, mexico going\n",
      "Topic 2:\n",
      "said going, long time, people know, great company, months ago, believe going, know look, art deal, thing going, stupid people\n",
      "Topic 3:\n",
      "great people, ll say, want tell, don worry, wall street, carl icahn, business people, don need, going use, really great\n",
      "Topic 4:\n",
      "united states, president obama, great job, money going, good people, know talking, los angeles, did great, folks going, taking money\n",
      "Topic 5:\n",
      "think going, people like, going say, let tell, look going, millions millions, good job, people think, way going, going end\n",
      "Topic 6:\n",
      "don want, make america, america great, want money, million people, south korea, make deal, white house, want people, great going\n",
      "Topic 7:\n",
      "don like, people say, going let, iran deal, lot things, don believe, say know, different things, going lot, run president\n",
      "Topic 8:\n",
      "didn want, new york, know doing, foreign policy, said know, know know, 50 million, going work, politically correct, york times\n",
      "Topic 9:\n",
      "going care, common core, things going, going start, common sense, country going, radical islamic, ve taken, care vets, islamic terrorism\n",
      "Topic 10:\n",
      "donald trump, 000 people, going great, millions dollars, self funding, said donald, people said, funding campaign, thought going, 20 000\n",
      "Topic 11:\n",
      "ve got, going win, years ago, don think, lot people, don win, going bring, make country, win going, jobs going\n",
      "Topic 12:\n",
      "mr trump, said oh, didn know, just want, people want, american people, just like, great deal, trump said, want say\n",
      "Topic 13:\n",
      "hillary clinton, special interests, thousands people, people come, radical islam, thousands thousands, great deals, said let, doesn want, know want\n",
      "Topic 14:\n",
      "new hampshire, don care, weeks ago, people people, saudi arabia, love love, going stop, doing great, incredible people, mean just\n",
      "Topic 15:\n",
      "people don, going going, trade deals, make great, want thank, going run, really good, free trade, think ll, don understand\n",
      "Topic 16:\n",
      "don know, lot money, know going, little bit, know ve, bring jobs, know don, just came, know people, going change\n",
      "Topic 17:\n",
      "south carolina, illegal immigration, ted cruz, world trade, time magazine, pretty good, good guy, nice guy, everybody agrees, want bring\n",
      "Topic 18:\n",
      "people going, going come, nuclear weapons, come country, going rid, smart people, good thing, built great, want know, trump administration\n",
      "Topic 19:\n",
      "second amendment, country great, period time, great guy, say going, said ll, 19 trillion, love people, know ll, american workers\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "print('\\nNMF top terms:')\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "print('\\nLDA top terms:')\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.022*\"want\" + 0.015*\"trade\" + 0.013*\"know\" + 0.013*\"even\" + 0.012*\"people\" '\n",
      "  '+ 0.012*\"fewer\" + 0.011*\"actually\" + 0.010*\"easy\" + 0.010*\"costs\" + '\n",
      "  '0.010*\"said\"'),\n",
      " (1,\n",
      "  '0.036*\"going\" + 0.030*\"They\" + 0.025*\"trade\" + 0.019*\"want\" + '\n",
      "  '0.017*\"afraid\" + 0.015*\"people\" + 0.014*\"cities\" + 0.014*\"talk\" + '\n",
      "  '0.013*\"report\" + 0.012*\"jobs\"'),\n",
      " (2,\n",
      "  '0.025*\"THEY\" + 0.021*\"TRUMP\" + 0.020*\"HAVE\" + 0.018*\"THAT\" + 0.018*\"GOING\" '\n",
      "  '+ 0.011*\"PEOPLE\" + 0.010*\"WHAT\" + 0.008*\"Indiana\" + 0.008*\"people\" + '\n",
      "  '0.008*\"SAID\"'),\n",
      " (3,\n",
      "  '0.032*\"know\" + 0.020*\"great\" + 0.020*\"evangelicals\" + 0.019*\"That\" + '\n",
      "  '0.018*\"rate\" + 0.018*\"Where\" + 0.016*\"unemployment\" + 0.014*\"said\" + '\n",
      "  '0.014*\"Sharon\" + 0.012*\"religion\"'),\n",
      " (4,\n",
      "  '0.037*\"THEY\" + 0.033*\"HAVE\" + 0.030*\"GOING\" + 0.029*\"THAT\" + 0.017*\"PEOPLE\" '\n",
      "  '+ 0.014*\"WHAT\" + 0.010*\"SAID\" + 0.010*\"THIS\" + 0.010*\"WITH\" + 0.010*\"WANT\"'),\n",
      " (5,\n",
      "  '0.022*\"people\" + 0.021*\"love\" + 0.019*\"know\" + 0.016*\"They\" + '\n",
      "  '0.014*\"dollars\" + 0.014*\"going\" + 0.014*\"along\" + 0.012*\"messenger\" + '\n",
      "  '0.011*\"millions\" + 0.011*\"gone\"'),\n",
      " (6,\n",
      "  '0.024*\"going\" + 0.019*\"jobs\" + 0.016*\"country\" + 0.014*\"need\" + '\n",
      "  '0.011*\"goods\" + 0.011*\"advantage\" + 0.010*\"president\" + 0.010*\"agree\" + '\n",
      "  '0.010*\"would\" + 0.010*\"Clinton\"'),\n",
      " (7,\n",
      "  '0.037*\"going\" + 0.030*\"people\" + 0.014*\"think\" + 0.013*\"President\" + '\n",
      "  '0.013*\"know\" + 0.012*\"work\" + 0.012*\"American\" + 0.010*\"growth\" + '\n",
      "  '0.010*\"NASCAR\" + 0.010*\"even\"'),\n",
      " (8,\n",
      "  '0.031*\"money\" + 0.022*\"know\" + 0.019*\"need\" + 0.016*\"campaign\" + '\n",
      "  '0.015*\"going\" + 0.013*\"tough\" + 0.013*\"year\" + 0.012*\"trade\" + '\n",
      "  '0.011*\"people\" + 0.011*\"history\"'),\n",
      " (9,\n",
      "  '0.024*\"Indiana\" + 0.018*\"foreign\" + 0.017*\"state\" + 0.016*\"going\" + '\n",
      "  '0.014*\"American\" + 0.014*\"Hillary\" + 0.012*\"great\" + 0.011*\"make\" + '\n",
      "  '0.010*\"made\" + 0.010*\"interests\"'),\n",
      " (10,\n",
      "  '0.034*\"Mike\" + 0.033*\"Thank\" + 0.023*\"people\" + 0.019*\"steel\" + '\n",
      "  '0.019*\"time\" + 0.015*\"want\" + 0.014*\"much\" + 0.014*\"America\" + '\n",
      "  '0.014*\"American\" + 0.012*\"back\"'),\n",
      " (11,\n",
      "  '0.025*\"jobs\" + 0.019*\"back\" + 0.018*\"They\" + 0.017*\"people\" + '\n",
      "  '0.015*\"budget\" + 0.015*\"bring\" + 0.014*\"know\" + 0.014*\"happened\" + '\n",
      "  '0.013*\"bankrupt\" + 0.012*\"plants\"'),\n",
      " (12,\n",
      "  '0.041*\"going\" + 0.023*\"said\" + 0.021*\"know\" + 0.018*\"like\" + 0.018*\"think\" '\n",
      "  '+ 0.015*\"want\" + 0.013*\"They\" + 0.013*\"great\" + 0.012*\"people\" + '\n",
      "  '0.012*\"never\"'),\n",
      " (13,\n",
      "  '0.138*\"going\" + 0.025*\"country\" + 0.022*\"make\" + 0.015*\"veterans\" + '\n",
      "  '0.014*\"great\" + 0.013*\"know\" + 0.012*\"said\" + 0.011*\"happen\" + '\n",
      "  '0.010*\"things\" + 0.010*\"powerful\"'),\n",
      " (14,\n",
      "  '0.026*\"people\" + 0.026*\"know\" + 0.023*\"said\" + 0.013*\"million\" + '\n",
      "  '0.013*\"City\" + 0.012*\"like\" + 0.011*\"going\" + 0.011*\"regulations\" + '\n",
      "  '0.011*\"want\" + 0.010*\"country\"'),\n",
      " (15,\n",
      "  '0.014*\"going\" + 0.013*\"years\" + 0.012*\"look\" + 0.012*\"know\" + '\n",
      "  '0.012*\"workers\" + 0.011*\"people\" + 0.011*\"horrible\" + 0.011*\"Clinton\" + '\n",
      "  '0.009*\"budgets\" + 0.009*\"American\"'),\n",
      " (16,\n",
      "  '0.035*\"said\" + 0.022*\"build\" + 0.016*\"building\" + 0.016*\"know\" + '\n",
      "  '0.014*\"They\" + 0.013*\"Pennsylvania\" + 0.013*\"never\" + 0.012*\"unbelievable\" '\n",
      "  '+ 0.011*\"like\" + 0.011*\"going\"'),\n",
      " (17,\n",
      "  '0.023*\"great\" + 0.022*\"Hillary\" + 0.020*\"Clinton\" + 0.017*\"going\" + '\n",
      "  '0.015*\"order\" + 0.014*\"people\" + 0.013*\"change\" + 0.011*\"leaders\" + '\n",
      "  '0.011*\"Korea\" + 0.010*\"thousands\"'),\n",
      " (18,\n",
      "  '0.025*\"workers\" + 0.017*\"said\" + 0.017*\"cars\" + 0.011*\"people\" + '\n",
      "  '0.011*\"take\" + 0.010*\"They\" + 0.010*\"Ukraine\" + 0.010*\"China\" + '\n",
      "  '0.010*\"away\" + 0.008*\"think\"'),\n",
      " (19,\n",
      "  '0.022*\"They\" + 0.022*\"people\" + 0.019*\"going\" + 0.015*\"allowed\" + '\n",
      "  '0.014*\"never\" + 0.011*\"believe\" + 0.011*\"police\" + 0.011*\"think\" + '\n",
      "  '0.011*\"seen\" + 0.011*\"know\"')]\n"
     ]
    }
   ],
   "source": [
    "# gensim LDA - может занять время\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "tok_collection = []\n",
    "for d in docss:\n",
    "    tok_collection.append([w for w in re.split('[\\W]+', d) if len(w) > 3 and w not in stopWords])\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(tok_collection)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in tok_collection]\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                           num_topics=n_topics,\n",
    "                                           id2word=dictionary)\n",
    "\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
