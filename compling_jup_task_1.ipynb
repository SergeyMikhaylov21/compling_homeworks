{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, clean_words, bigrams_list = [], [], []\n",
    "with open('russian_stopwords_nltk.txt', encoding='utf-8') as nltkstops:\n",
    "    stopwords = nltkstops.read().splitlines()\n",
    "\n",
    "with open('rus-wikipedia-sample-companies.txt', encoding = 'utf-8') as corpusfile:\n",
    "    lines = corpusfile.read().splitlines()\n",
    "    for line in lines:\n",
    "        wrd = line.split(' ')\n",
    "        for elem in range(len(wrd)):\n",
    "            x = wrd[elem].strip().replace(',', '').replace(' ', '').replace('-', '').replace('(', '')\n",
    "            y = x.replace(')', '').replace('.', '').lower()\n",
    "            words.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for elem in words: #убираем стоп-слова, используя список стоп-слов для русского языка от nltk\n",
    "    if elem not in stopwords:\n",
    "        clean_words.append(elem)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bigrams(input_list): #находим все биграммы\n",
    "  return zip(input_list, input_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams = find_bigrams(clean_words) #создаем список биграмм и считаем количество биграмм и слов\n",
    "bigrams_quantity = 0\n",
    "for elem in bigrams:\n",
    "    bigrams_list.append(elem)\n",
    "    bigrams_quantity += 1\n",
    "token_quantity = len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "facebook_list, apple_list, google_list, amazon_list = [], [], [], [] #выбираем только нужные нам биграммы\n",
    "for i in bigrams_list:\n",
    "    for j in i:\n",
    "        if str(j) == 'google' and '' not in i:\n",
    "            google_list.append(i)\n",
    "        elif str(j) == 'facebook' and '' not in i:\n",
    "            facebook_list.append(i)\n",
    "        elif str(j) == 'apple' and '' not in i:\n",
    "            apple_list.append(i)\n",
    "        elif str(j) == 'amazon' and '' not in i:\n",
    "            amazon_list.append(i)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_1 = collections.Counter() #считаем значения топ-20 биграмм для каждого из искомых слов\n",
    "c_2 = collections.Counter()\n",
    "c_3 = collections.Counter()\n",
    "c_4 = collections.Counter()\n",
    "for colloc_1 in facebook_list:\n",
    "    c_1[colloc_1] += 1\n",
    "    facebook_top = c_1.most_common(20)\n",
    "for colloc_2 in apple_list:\n",
    "    c_2[colloc_2] += 1\n",
    "    apple_top = c_2.most_common(20)\n",
    "for colloc_3 in google_list:\n",
    "    c_3[colloc_3] += 1\n",
    "    google_top = c_3.most_common(20)\n",
    "for colloc_4 in amazon_list:\n",
    "    c_4[colloc_4] += 1\n",
    "    amazon_top = c_4.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_5 = collections.Counter() #считаем частоту всех слов в данных текстах\n",
    "for cl_w in clean_words:\n",
    "    c_5[cl_w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('collocations_results.txt', 'a', encoding='utf-8') as final:\n",
    "    final.write('word1'+'\\t'+'word2'+'\\t'+'count(word1)'+'\\t'+'count(word2)'+'\\t'+'count(bigram)'+'\\t'+'PMI'+'\\n')\n",
    "    for i in facebook_top:\n",
    "        word1_name = i[0][0]\n",
    "        word2_name = i[0][1]\n",
    "        word1_count = int(c_5[word1_name])\n",
    "        word2_count = int(c_5[word2_name])\n",
    "        w1w2bigram_count = int(i[1])\n",
    "        pmi = math.log2(w1w2bigram_count / (bigrams_quantity-1)) / (math.log2(word1_count / token_quantity) * math.log2(word2_count / token_quantity))\n",
    "        final.write(str(word1_name)+'\\t'+str(word2_name)+'\\t'+str(word1_count)+'\\t'+str(word2_count)+'\\t'+str(w1w2bigram_count)+'\\t'+str(round(pmi, 3))+'\\n')\n",
    "    final.write('\\n')\n",
    "    for k in apple_top:\n",
    "        word1_name = k[0][0]\n",
    "        word2_name = k[0][1]\n",
    "        word1_count = int(c_5[word1_name])\n",
    "        word2_count = int(c_5[word2_name])\n",
    "        w1w2bigram_count = int(i[1])\n",
    "        pmi = math.log2(w1w2bigram_count / (bigrams_quantity-1)) / (math.log2(word1_count / token_quantity) * math.log2(word2_count / token_quantity))\n",
    "        final.write(str(word1_name)+'\\t'+str(word2_name)+'\\t'+str(word1_count)+'\\t'+str(word2_count)+'\\t'+str(w1w2bigram_count)+'\\t'+str(round(pmi, 3))+'\\n')\n",
    "    final.write('\\n')\n",
    "    for z in google_top:\n",
    "        word1_name = z[0][0]\n",
    "        word2_name = z[0][1]\n",
    "        word1_count = int(c_5[word1_name])\n",
    "        word2_count = int(c_5[word2_name])\n",
    "        w1w2bigram_count = int(i[1])\n",
    "        pmi = math.log2(w1w2bigram_count / (bigrams_quantity-1)) / (math.log2(word1_count / token_quantity) * math.log2(word2_count / token_quantity))\n",
    "        final.write(str(word1_name)+'\\t'+str(word2_name)+'\\t'+str(word1_count)+'\\t'+str(word2_count)+'\\t'+str(w1w2bigram_count)+'\\t'+str(round(pmi, 3))+'\\n')\n",
    "    final.write('\\n')\n",
    "    for t in amazon_top:\n",
    "        word1_name = t[0][0]\n",
    "        word2_name = t[0][1]\n",
    "        word1_count = int(c_5[word1_name])\n",
    "        word2_count = int(c_5[word2_name])\n",
    "        w1w2bigram_count = int(i[1])\n",
    "        pmi = math.log2(w1w2bigram_count / (bigrams_quantity-1)) / (math.log2(word1_count / token_quantity) * math.log2(word2_count / token_quantity))\n",
    "        final.write(str(word1_name)+'\\t'+str(word2_name)+'\\t'+str(word1_count)+'\\t'+str(word2_count)+'\\t'+str(w1w2bigram_count)+'\\t'+str(round(pmi, 3))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
